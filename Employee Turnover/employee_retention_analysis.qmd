---
title: "Employee Turnover Analysis"
format:
  html:
    theme: minty
    highlight-style: kate
    fontsize: 9pt
    embed-resources: true
    self-contained-math: true
    code-fold: true
    code-summary: "Show the code"
    code-overflow: wrap
    code-block-bg: true
    code-block-border-left: "#31BAE9"
    code-line-numbers: false
toc: true
toc-title: Contents
toc-location: right
toc-depth: 4
number-sections: true
code-line-numbers: true
code-fold: false
papersize: "letterpaper"
editor: visual
code-block-bg: true
title-block-banner: true
title-block-banner-color: lightgrey
---

# Introduction

Employee retention is a critical factor for organizational success, impacting productivity, morale, and financial costs. High turnover rates can disrupt team dynamics, increase recruitment and training expenses, and lead to knowledge loss. This analysis explores the factors influencing employee turnover, using a dataset obtained from Kaggle, to uncover patterns that may help organizations retain their talent more effectively.

The dataset includes various metrics related to employee satisfaction, performance, workload, tenure, and compensation. By examining these variables, we aim to identify key drivers of retention and attrition, providing actionable insights for HR teams and managers to develop targeted retention strategies.

# Analysis Objectives

In this analysis, we will:

-   **Explore**: Investigate the dataset to understand the distributions and relationships among different variables.

-   **Model**: Use statistical and predictive modeling to identify factors strongly associated with employee turnover.

-   **Interpret**: Derive insights and recommendations for retention strategies based on the findings.

Skills highlighted:

-   Data wrangling, logistic regression, neural networks, visualization, R.

# Dataset Overview

The data comprises 14,999 employee records with the following variables:

-   **satisfaction_level**: Employee satisfaction score, ranging from 0 to 1.

-   **last_evaluation**: Most recent performance evaluation score, ranging from 0 to 1.

-   **number_project**: Number of projects the employee has worked on.

-   **average_montly_hours**: Average number of hours worked per month.

-   **time_spend_company**: Number of years with the company.

-   **Work_accident**: Whether the employee experienced a work accident (1 = yes, 0 = no).

-   **left**: Indicates if the employee has left the company (1 = yes, 0 = no).

-   **promotion_last_5years**: Whether the employee received a promotion in the last five years.

-   **sales**: Department of employment, such as "sales," "technical," etc.

-   **salary**: Salary level categorized as "low," "medium," or "high."

```{r initial setup, echo=TRUE}

#-----------------------------------------------------------------------------
# Initial Setup & Packages ---------------------------------------------------
#-----------------------------------------------------------------------------

# Clear Environment
rm(list=ls())

options(digits = 4)
options(max.print = 2000)

#loading packages
pacman::p_load(readxl,
               tidyverse, 
               ggplot2,
               highcharter,
               caret,
               randomForest,
               corrplot,
               Hmisc,
               tidymodels,
               finalfit,
               performance,
               pscl, car)

# Turning off scientific notation
options(scipen = 999)

#-----------------------------------------------------------------------------
# Importing Data -------------------------------------------------------------
#-----------------------------------------------------------------------------

employee_retention_data <- read_excel("C:/Users/ClaudieCoulombe/OneDrive - System 3/General P/Employee Retention/hr_retention_data.xlsx")

```

# Data Preparation

## Checking for Missing Values

```{r data preparation missing values, echo=TRUE}

#-----------------------------------------------------------------------------
# Check for Missing Values ---------------------------------------------------
#-----------------------------------------------------------------------------

sapply(employee_retention_data, function(x) sum(is.na(x)))

```

The variables do not have missing values.

## Checking Data Types

```{r data preparation data types, echo=TRUE}

#-----------------------------------------------------------------------------
# Check Variables and Data Types ---------------------------------------------
#-----------------------------------------------------------------------------

# Check data types
str(employee_retention_data)

```

We can see that some categorical variables (i.e., left, sales, salary) are stored as the character data type, when they should be stored as factors. We convert that in the next step.

```{r data preparation data type correction, echo=TRUE}

# Convert categorical variables (left, sales, salary) to factors
employee_retention_data$left <- as.factor(employee_retention_data$left)
employee_retention_data$salary <- as.factor(employee_retention_data$salary)
employee_retention_data$sales <- as.factor(employee_retention_data$sales)

# Check data types
str(employee_retention_data)

```

We can also see that there is a typo in some of the variable names. Specifically, average_montly_hours and Work_accident. We correct those in the code below.

```{r data preparation typo correction, echo=TRUE}

#-----------------------------------------------------------------------------
# Correct Typo in Variable Name ----------------------------------------------
#-----------------------------------------------------------------------------

employee_retention_data <- employee_retention_data %>%
  rename(average_monthly_hours = average_montly_hours,
         work_accident = Work_accident)

# Verify that the name correction worked. 
str(employee_retention_data)
```

# Exploratory Data Analysis (EDA)

## Basic Descriptive Statistics

For numeric variables, we calculate the minimum, 1st quartile, median, mean, 3rd quartile, and maximum.

```{r summarizing numeric variables, echo=TRUE}

# For numeric variables, calculate: min, 1st quartile, median, mean, 3rd quartile, and max.
summary(dplyr::select(employee_retention_data, where(is.numeric)))

```

For categorical variables, we find the count for each level.

```{r summarizing categorical variables, echo=TRUE}

# Get counts for each level in each factor variable

xtabs(~ left, data=employee_retention_data)

xtabs(~ salary, data=employee_retention_data)

xtabs(~ sales, data=employee_retention_data)
```

## Outliers Check

In this section, we examine the distribution of key numeric variables using boxplots to identify any potential outliers or unusual patterns. Each boxplot displays the distribution of values for a specific variable, helping us understand central tendencies and variability.

```{r checking for outliers in numeric variables, echo=TRUE}

# Reshape data into long format for faceting
numeric_data_long <- employee_retention_data %>%
  dplyr::select(where(is.numeric), -c(promotion_last_5years, work_accident)) %>%
  pivot_longer(cols = everything(), names_to = "variable", values_to = "value")

# Create faceted boxplots
ggplot(numeric_data_long, aes(y = value)) +
  geom_boxplot(fill = "skyblue") +
  facet_wrap(~ variable, scales = "free_y") +
  labs(title = "Box Plots of Numeric Variables", x = "Variable", y = "Value") +
  theme_minimal() +
  theme(axis.text.x = element_blank())
```

::: {.callout-important collapse="true" icon="false"}
## Key Observations

1.  **Average Monthly Hours**:

    -   The distribution of monthly hours appears fairly centralized, with no significant outliers.

    -   Most employees have monthly hours between 150 and 250, suggesting a generally consistent work schedule across the dataset.

2.  **Last Evaluation**:

    -   Scores for the last evaluation are relatively high, generally clustering between 0.6 and 0.9.

    -   No obvious outliers are present, indicating that most employees receive evaluations within a similar range.

3.  **Number of Projects**:

    -   The number of projects ranges from 2 to 7, with a central concentration around 3 to 5 projects.

    -   There are no extreme outliers, suggesting that employees’ project loads are fairly consistent.

4.  **Satisfaction Level**:

    -   Satisfaction scores span the full range from 0 to 1, indicating diverse levels of employee satisfaction.

    -   The distribution is slightly skewed toward higher satisfaction scores, with no obvious outliers.

5.  **Time Spent at the Company**:

    -   This variable shows some notable outliers, with a few employees spending significantly longer at the company than the majority.

    -   **Real-World Note**: In a real-world scenario, we would investigate these high values to confirm whether they represent actual long-tenured employees or potential data entry errors. For the purposes of this analysis, we’ll assume that these values are valid and reflect long-tenured employees.
:::

## Correlations

In this analysis, we examine the relationships between various continuous variables (e.g., satisfaction level, last evaluation) and a binary variable (`left`, indicating turnover) using correlation. When calculating correlations between continuous variables, we use **Pearson correlation**. For binary-continuous variable pairs, we use **point-biserial correlation**, which is technically a special case of Pearson correlation equivalent to running the Pearson's correlation when you have one continuous and one dichotomous variable.

```{r correlations, echo=TRUE}

#-----------------------------------------------------------------------------
# Create a Correlation Matrix Heatmap ----------------------------------------
#-----------------------------------------------------------------------------

# Convert 'left' to numeric for correlation analysis
employee_retention_data$left_numeric <- as.numeric(as.character(employee_retention_data$left))

# Select relevant continuous variables
correlation_data <- employee_retention_data %>%
  dplyr::select(satisfaction_level, last_evaluation, average_monthly_hours, number_project, time_spend_company, left_numeric)

# Run the correlation matrix with p-values
cor_results <- rcorr(as.matrix(correlation_data))
cor_matrix <- round(cor_results$r, 2)  # Correlation coefficients
p_matrix <- cor_results$P              # P-values

# Convert correlation and p-value matrices to long format
cor_long <- as.data.frame(as.table(cor_matrix))
p_long <- as.data.frame(as.table(p_matrix))
names(cor_long) <- c("x", "y", "value")
names(p_long) <- c("x", "y", "p_value")

# Merge the correlation and p-value data frames
cor_data <- merge(cor_long, p_long, by = c("x", "y"))

# Add significance stars based on p-values
cor_data <- cor_data %>%
  mutate(
    significance = case_when(
      p_value < 0.001 ~ "***",
      p_value < 0.01 ~ "**",
      p_value < 0.05 ~ "*",
      TRUE ~ ""
    ),
    label = paste0(round(value, 2), significance)  # Combine value and significance into one label
  )

# Create the heatmap with highcharter
highchart() %>%
  hc_add_series(data = cor_data, type = "heatmap", hcaes(x = x, y = y, value = value)) %>%
  hc_colorAxis(stops = color_stops(colors = c("#6D9EC1", "white", "#E46726"))) %>%  # Color gradient
  hc_title(text = "Correlation Matrix of Employee Retention Variables") %>%
  hc_tooltip(pointFormat = "{point.x} and {point.y}: {point.label}") %>%  # Tooltip with correlation and stars
  hc_xAxis(categories = colnames(cor_matrix), title = list(text = NULL)) %>%
  hc_yAxis(categories = colnames(cor_matrix), title = list(text = NULL), reversed = TRUE) %>%
  hc_plotOptions(heatmap = list(dataLabels = list(enabled = TRUE, format = '{point.label}')))  # Show labels with stars
```

*Note.* `***` for p \< 0.001, `**` for p \< 0.01, `*` for p \< 0.05.

::: {.callout-important collapse="true" icon="false"}
## Key Findings

1.  **Employee Satisfaction and Turnover**: The correlation coefficient of **-0.39**\*\*\* indicates a significant negative relationship between employee satisfaction and turnover. This suggests that as employee satisfaction decreases, the likelihood of employees leaving the organization increases.
2.  **Number of Projects and Turnover**: The correlation coefficient is **-0.14\*\*\***, indicating a significant negative relationship between number of projects and turnover (however, this relationship appears weaker than the relationship between satisfaction and turnover).
3.  **Last Evaluation and Turnover:** The correlation coefficient is **0.11\*\*\***, indicating a significant positive relationship between performance on the last evaluation and turnover. Perhaps high-performing employees are those who are more likely to receive job offers elsewhere.
4.  **Time Spent at Company and Turnover:** The correlation coefficient is **-0.10\*\*\***, indicating a significant negative relationship. This suggests that employees who have spent less time at the company may be more likely to leave.
:::

## Distributions of Key Variables

Correlations can be helpful, but they sometimes don't tell the full story. To get a better sense of the shape of the data, it can be helpful to inspect it visually. In this section, we produce histograms displaying the distribution of key variables, grouped by turnover status. This helps us get a visual sense of the shape of the data, as well as spot any starting trends.

```{r distributions, echo=TRUE}

# Set theme_classic() as the default theme

theme_set(
  theme_classic() + 
    theme(legend.position = "right")
  )

#-----------------------------------------------------------------------------
# Satisfaction Level by Turnover ---------------------------------------------
#-----------------------------------------------------------------------------

hist_sat <- ggplot(employee_retention_data, aes(x = satisfaction_level)) +
  geom_histogram(aes(color = left, fill = left), 
                 position = "identity", bins = 30, alpha = 0.4) +
  scale_color_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  scale_fill_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  labs(
    title = "Employee Satisfaction by Turnover Status",  
    x = "Satisfaction Level",                             
    y = "Count"                          
  )

hist_sat

#-----------------------------------------------------------------------------
# Employee Workload by Turnover -------------------------------------------
#-----------------------------------------------------------------------------

hist_perf <- ggplot(employee_retention_data, aes(x = last_evaluation)) +
  geom_histogram(aes(color = left, fill = left), 
                 position = "identity", bins = 30, alpha = 0.4) +
  scale_color_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  scale_fill_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  labs(
    title = "Employee Performance on Last Evaluation by Turnover Status",  
    x = "Performance Rating on Last Evaluation",                             
    y = "Count"                          
  )

hist_perf

#-----------------------------------------------------------------------------
# Employee Workload (Monthly Hours) by Turnover ------------------------------
#-----------------------------------------------------------------------------

hist_avg_hours <- ggplot(employee_retention_data, aes(x = average_monthly_hours)) +
  geom_histogram(aes(color = left, fill = left), 
                 position = "identity", bins = 30, alpha = 0.4) +
  scale_color_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  scale_fill_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  labs(
    title = "Employee Workload (Based on Average Monthly Hours) by Turnover Status",  
    x = "Average Monthly Hours",                             
    y = "Count"                          
  )

hist_avg_hours

#-----------------------------------------------------------------------------
# Employee Workload (Number of Projects) by Turnover -------------------------
#-----------------------------------------------------------------------------

hist_num_projects <- ggplot(employee_retention_data, aes(x = number_project)) +
  geom_histogram(aes(color = left, fill = left), 
                 position = "identity", bins = 30, alpha = 0.4) +
  scale_color_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  scale_fill_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  labs(
    title = "Employee Workload (Based on Number of Projects) by Turnover Status",  
    x = "Number of Projects",                             
    y = "Count"                          
  )

hist_num_projects

#-----------------------------------------------------------------------------
# Time Spent at Company by Turnover ------------------------------------------
#-----------------------------------------------------------------------------

hist_time_spent <- ggplot(employee_retention_data, aes(x = time_spend_company)) +
  geom_histogram(aes(color = left, fill = left), 
                 position = "identity", bins = 30, alpha = 0.4) +
  scale_color_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  scale_fill_manual(values = c("#00AFBB", "#E7B800"), labels = c("Stayed", "Left"), name = "Turnover Status") +
  labs(
    title = "Time Spent at Company by Turnover Status",  
    x = "Number of Years at Company",                             
    y = "Count"                          
  )

hist_time_spent
```

::: {.callout-important collapse="true" icon="false"}
## Key Observations

Purely based on visual inspection of the histograms:

1.  **Satisfaction Level:** There may be a difference in satisfaction levels between employees who stayed and those who left. A large proportion of employees who left have satisfaction scores below 0.50. There also appears to be higher and more consistent satisfaction scores among employees who stayed. Overall, this suggests lower satisfaction levels may be related to higher turnover, and this may be an important variable to further examine.

2.  **Employee Performance on Last Evaluation:** Among employees who left, there appears to be a U-shaped pattern in turnover, with clusters at both lower and higher ends of the performance scale. This "U-shaped" pattern could suggest that turnover may be more common among employees with either low or high performance ratings, while those with mid-range performance ratings (approximately 0.6 to 0.8) may be more likely to stay. However, it's important to note that there are still a high number of employees who stayed across all performance levels, including those on the lower and upper end. This suggests that performance ratings alone may not capture the full story.

3.  **Workload (Based on Average Monthly Hours):** Employees who left tend to cluster at both ends of the workload spectrum. Those working very few hours (around 150 or below) and those working very high hours (above around 250) show higher turnover rates. This pattern suggests that both under-utilization and overwork may be associated with a higher likelihood of turnover.

4.  **Workload (Based on Number of Projects**): Employees who left tend to have either very few projects (e.g., 2) or a high number (6 or 7). The majority of employees who stayed tend to have a mid-range number of projects. This pattern suggests that both under-engagement and over-engagement could relate to turnover.

5.  **Time Spent at the Company:** There is a spike in turnover among employees with fewer years at the company, particularly around 2 to 3 years. This suggests that employees may be more likely to leave within their first few years of employment. This is consistent with the correlation we observed.
:::

# Logistic Regression

Logistic regression is used to model the relationship between one or more independent variables (predictors) and a binary dependent variable (outcome). Unlike linear regression, which predicts a continuous outcome (like height or salary), logistic regression predicts probabilities for binary outcomes. It estimates the likelihood that an event occurs (e.g., an employee leaving) based on the predictors.

-   Logistic regression works by estimating **odds**. The odds are a way of expressing the likelihood of an event happening compared to it not happening. For example, if the odds of an employee leaving are 3:1, it means they are three times more likely to leave than they are to not leave (i.e., to stay).

-   Logistic regression uses the **logit function** to convert these odds into probabilities that range between 0 and 1. This is done using the logistic function, which ensures that the predicted probabilities always fall within this range.

-   The coefficients of the logistic regression indicate how much the log odds of the outcome change with a one-unit increase in the predictor.

## Splitting the Data

The first thing we'll do is split our data into a training dataset and a testing dataset. We do this by randomly selecting 70% of the data to go into the training dataset and the remaining 30% to go into our test dataset.

We'll use the training dataset to build the model and the testing dataset to evaluate it after it's been trained, to see how well it predicts new data it hasn't seen before.

This will help us get a sense of the extent to which the model we've build generalizes to new data. In other words, it'll **help prevent overfitting** (where the model learns the training data too well and fails to generalize to new data) and help us flag possible issues before this model is deployed in real-world settings.

```{r splitting the data, echo=TRUE}

# Split the data (e.g., 70% training, 30% test)

set.seed(123)  # for reproducibility

train_index <- createDataPartition(employee_retention_data$left_numeric, p = 0.7, list = FALSE)

train_data <- employee_retention_data[train_index, ]

test_data <- employee_retention_data[-train_index, ]

```

## Assumption Checking

The main assumptions for logistic regression are:

**Binary Outcome Variable**: The dependent variable must be binary. In our case, this is represented by whether an employee left (1) or stayed (0).

**Independence of Observations**: Each observation should be independent of the others. This means that the response for one participant should not influence the response of another. This is typically ensured through appropriate study design (e.g., random sampling).

**Linearity in the Logit**: The relationship between the continuous predictors and the log odds of the outcome should be linear. This does not mean that the continuous predictors themselves must be linearly related to the outcome; rather, the logit transformation of the outcome should have a linear relationship with the continuous predictors.

-   Note: The linearity assumption is important for continuous predictors, because they can take on a wide range of values and the model expects that as the predictor(s) increase or decrease, they'll have a consistent, predictable impact on the log odds of the outcome (e.g., if satisfaction level goes up, it'll have a predictable effect on the log odds of turnover in a linear way). If the relationship is not linear, then the model's assumptions are violated which can affect the accuracy and reliability of predictions. If the model's assumptions are violated, we might need to transform the predictor(s) or use categories instead.

    -   By converting the continuous predictors into categories, we're getting rid of their continuous range by categorizing them into separate, distinct groups (e.g., "low," "medium," "high). Each of these categories gets treated as its own unique group, without any assumed relationships between them. The model simply compares the outcome for each group against a reference group (e.g., comparing "medium" and "high" to "low). Since each category is separate, the model doesn't expect a continuous, linear relationship with the log odds of the outcome; it just sees each category as a different scenario.

    -   In short, **linearity only matters for continuous predictors** because they have a range of values, and we want to see if there's a straight-line trend between those values and the outcome. For categories, since they're treated as distinct groups, we don't need a straight-line trend.

*To check the linearity assumption, we can plot each predictor against the logit (the log odds) of the outcome. If the linearity assumption is met, we should expect to see a relatively straight line.*

```{r testing linearity, echo=TRUE}

#-----------------------------------------------------------------------------
# Create the Logistic Regression Model ---------------------------------------
#-----------------------------------------------------------------------------

outcome <- "left_numeric"

predictors <- c("satisfaction_level", "last_evaluation", "average_monthly_hours", 
                "number_project", "time_spend_company")

# Train the model on training data
model <- glm(left_numeric ~ satisfaction_level + last_evaluation + 
             average_monthly_hours + number_project + 
             time_spend_company, 
             data = train_data, 
             family = binomial)

#-----------------------------------------------------------------------------
# Extract Pobabilities and Logits --------------------------------------------
#-----------------------------------------------------------------------------

probabilities <- predict(model, type = "response")

logit <- log(probabilities/(1-probabilities))

#-----------------------------------------------------------------------------
# Create Plots to Check Linearity Assumption for Each Continuous Predictor ---
#-----------------------------------------------------------------------------

ggplot(train_data, aes(logit,satisfaction_level))+
  geom_point(size=0.5, alpha=0.5) + 
  geom_smooth(method="loess") +
  theme_bw()

ggplot(train_data, aes(logit,last_evaluation))+
  geom_point(size=0.5, alpha=0.5) + 
  geom_smooth(method="loess") +
  theme_bw()

ggplot(train_data, aes(logit,average_monthly_hours))+
  geom_point(size=0.5, alpha=0.5) + 
  geom_smooth(method="loess") +
  theme_bw()

ggplot(train_data, aes(logit,number_project))+
  geom_point(size=0.5, alpha=0.5) + 
  geom_smooth(method="loess") +
  theme_bw()

ggplot(train_data, aes(logit,time_spend_company))+
  geom_point(size=0.5, alpha=0.5) + 
  geom_smooth(method="loess") +
  theme_bw()

```

From these plots, we can see that none of the predictor variables seem to meet the linearity assumption. They all have some kind of curve.

We could try transforming the predictors to see if it helps; but in our case, we'll just convert them to categorical variables.

```{r converting to categories, echo=TRUE}

#-----------------------------------------------------------------------------
# Convert Continuous Predictors to Factor Variables --------------------------
#-----------------------------------------------------------------------------

# We'll try categorizing the predictors into low, medium, and high levels and treating them as categorical variables. We'll use quantiles to define cutoffs. 

train_data <- train_data %>%
  mutate(
    satisfaction_level_cat = case_when(
      satisfaction_level <= quantile(satisfaction_level, 0.25) ~ "Low",
      satisfaction_level > quantile(satisfaction_level, 0.25) & satisfaction_level <= quantile(satisfaction_level, 0.50) ~ "Lower-Mid",
      satisfaction_level > quantile(satisfaction_level, 0.50) & satisfaction_level <= quantile(satisfaction_level, 0.75) ~ "Upper-Mid",
      satisfaction_level > quantile(satisfaction_level, 0.75) ~ "High"
    ),
    last_evaluation_cat = case_when(
      last_evaluation <= quantile(last_evaluation, 0.25) ~ "Low",
      last_evaluation > quantile(last_evaluation, 0.25) & last_evaluation <= quantile(last_evaluation, 0.50) ~ "Lower-Mid",
      last_evaluation > quantile(last_evaluation, 0.50) & last_evaluation <= quantile(last_evaluation, 0.75) ~ "Upper-Mid",
      last_evaluation > quantile(last_evaluation, 0.75) ~ "High"
    ),
    average_monthly_hours_cat = case_when(
      average_monthly_hours <= quantile(average_monthly_hours, 0.25) ~ "Low",
      average_monthly_hours > quantile(average_monthly_hours, 0.25) & average_monthly_hours <= quantile(average_monthly_hours, 0.50) ~ "Lower-Mid",
      average_monthly_hours > quantile(average_monthly_hours, 0.50) & average_monthly_hours <= quantile(average_monthly_hours, 0.75) ~ "Upper-Mid",
      average_monthly_hours > quantile(average_monthly_hours, 0.75) ~ "High"
    ),
    number_project_cat = case_when(
      number_project <= quantile(number_project, 0.25) ~ "Low",
      number_project > quantile(number_project, 0.25) & number_project <= quantile(number_project, 0.50) ~ "Lower-Mid",
      number_project > quantile(number_project, 0.50) & number_project <= quantile(number_project, 0.75) ~ "Upper-Mid",
      number_project > quantile(number_project, 0.75) ~ "High"
    ),
    time_spend_company_cat = case_when(
      time_spend_company <= quantile(time_spend_company, 0.25) ~ "Low",
      time_spend_company > quantile(time_spend_company, 0.25) & time_spend_company <= quantile(time_spend_company, 0.50) ~ "Lower-Mid",
      time_spend_company > quantile(time_spend_company, 0.50) & time_spend_company <= quantile(time_spend_company, 0.75) ~ "Upper-Mid",
      time_spend_company > quantile(time_spend_company, 0.75) ~ "High"
    )
  )

# Convert to factors
train_data <- train_data %>%
  mutate(across(ends_with("_cat"), as.factor))

```

-   **Absence of Multicollinearity**: Predictors should not be too highly correlated with each other. High multicollinearity can make it difficult to assess the effect of each predictor.

    -   This can be checked by looking at the correlation matrix. As a rule of thumb, anything above 0.7 would be too highly correlated. We would remove those from the model since they would be redundant. From our matrix above, we can see that the predictors don't seem to be too highly correlated.

    -   This can also be checked using Variance Inflation Factor (VIF) scores, where VIF values above 5 or 10 indicate a potential multicollinearity problem. As seen below, all the VIF values are below 5, suggesting there is no multicollinearity problem.

*In the code below, we calculate the VIF for our predictors.*

```{r testing vif, echo=TRUE}

# Check the variabce inflation factor scores
vif(model)
```

-   **No Outliers**: While not a formal assumption, logistic regression can be sensitive to outliers. Outliers can disproportionately affect the model's estimates and should be examined before fitting the model.

-   **Large Sample Size**: Logistic regression generally requires a larger sample size to provide reliable estimates. A rule of thumb is to have at least 10 events (e.g., "1" outcomes) per predictor in the model. This helps ensure stability in the estimates.

*In the code below, we check if the sample size is large enough by calculating the number of cases of turnover divided by the number of predictors.*

```{r sample size rule of thumb, echo=TRUE}

# Count the number of "1" outcomes 
left_count <- sum(employee_retention_data$left_numeric == 1)

# Number of predictors (update as needed)
predictors_count <- length(c("satisfaction_level", "last_evaluation", "average_monthly_hours", "number_project", "time_spend_company"))

# Divide the number of "1" outcomes by the number of predictors
left_per_predictor <- left_count / predictors_count
left_per_predictor
```

-   The events per predictor ratio is 714.2, which exceeds the rule of thumb of 10 per predictor.

## Fitting the Model

In this section, we build our logistic regression model using our training data. We regress the binary outcome, left, on the categorical predictors we created above.

For each categorical variable, R automatically sets one level as the **reference category** (usually alphabetically first unless specified). All other levels are interpreted relative to this baseline. In our case, R selected the "high" categories as the reference groups.

The results are displayed in terms of odds ratios, which get calculated from the model coefficients. The model outputs log-odds coefficients, but these are not as interpretable as odds ratios, and so we present the odds ratios below.

-   **Interpretation**: If an odds ratio is above 1, it means an increase in that predictor increases the odds of the outcome (e.g., leaving), while values below 1 suggest a decrease in odds.

```{r fit the logistic regression on training dataset, echo=TRUE}

# Train the model on training data
logistic_model <- glm(left_numeric ~ satisfaction_level_cat + last_evaluation_cat +
             average_monthly_hours_cat + number_project_cat + 
             time_spend_company_cat, 
             data = train_data, 
             family = binomial)

suppressWarnings({
library(sjPlot)
})


suppressMessages({
  
tab_model(logistic_model, 
          title = "Logistic Regression Model Results",
          show.ci = FALSE,   
          show.se = TRUE,    
          show.stat = TRUE,
          show.reflvl = TRUE,
          prefix.labels = "varname",
          CSS = list(
            css.depvarhead = 'text-align: left;',
            css.centeralign = 'text-align: left;',
            css.summary = 'font-weight: bold;'
          ))
  })

```

::: {.callout-important collapse="true" icon="false"}
### Results

**(Intercept)**:

-   This is the baseline odds when all categorical variables are in their reference categories. An odds ratio of 1.98 means that the baseline odds of the outcome (e.g., "leaving") are about 1.98 times the odds of not leaving when all other predictors are at their reference levels ("high").

**Satisfaction Level:**

-   `Low`: An odds ratio of 7.46 indicates that employees with a "Low" satisfaction level have approximately 7.46 times the odds of leaving compared to the those with a "High" satisfaction level.

-   `Lower-Mid`: An odds ratio of 0.66 suggests that the odds of leaving for employees in the "Lower-Mid" satisfaction level are about 0.66 times the odds of leaving compared to the those with a "High" satisfaction level. This is less than 1, indicating a lower likelihood compared to the reference.

-   `Upper-Mid`: An odds ratio of 1.14 means the odds of leaving for "Upper-Mid" satisfaction level employees are 1.14 times the odds of leaving compared to the those with a "High" satisfaction level, suggesting a slightly increased likelihood.

**Performance at Last Evaluation**:

-   `Low`: An odds ratio of 0.98 shows almost no difference in odds of leaving for those with "Low" performance compared to those with "High" performance.

-   `Lower-Mid`: An odds ratio of 0.12 indicates a much lower likelihood of leaving for those with Lower-Mid performance compared to those with "High" performance.

-   `Upper-Mid`: An odds ratio of 0.45 suggests the odds of those with Upper-Mid performance leaving are about 0.45 times those of employees with "High" performance, indicating a reduced likelihood.

**Average Monthly Hours:**

-   `Low`: An odds ratio of 0.82 suggests employees in the "Low" average monthly hours category have slightly lower odds of leaving compared to employees with "High" average monthly hours.

-   `Lower-Mid`: An odds ratio of 0.15 shows a much lower likelihood of leaving compared to employees with "High" average monthly hours.

-   `Upper-Mid`: An odds ratio of 0.40 indicates lower odds of leaving compared to employees with "High" average monthly hours.

**Number of Projects**:

-   `Low`: An odds ratio of 0.75 suggests employees in the "Low" project count category have slightly lower odds of leaving compared to employees with a "High" project count.

-   `Lower-Mid`: An odds ratio of 0.28 shows a much lower likelihood of leaving compared to employees with a "High" project count.

-   `Upper-Mid`: An odds ratio of 0.62 indicates slightly lower odds of leaving compared to employees with a "High" project count.

**Time Spent at Company:**

-   `Low`: An odds ratio of 0.28 suggests employees in the "Low" time-spent category have a lower likelihood of leaving compared to employees in the "High" time-spent category.

-   `Upper-Mid`: An odds ratio of 0.36 shows lower odds of leaving for those in the "Upper-Mid" category than those in the "High" category.
:::

### Evaluating Model Fit

**AIC (Akaike Information Criterion)**: This is a commonly used metric for model fit in logistic regression. Lower AIC values indicate a better fit. You can compare AIC values between different models to see which model fits the data better.

```{r model fit index AIC, echo=TRUE}

AIC(logistic_model)
```

**Pseudo R-squared**: Unlike linear regression, there’s no true R-squared for logistic regression, but you can use pseudo R-squared values as an approximation of model fit. Examples include **McFadden's R-squared**, **Cox & Snell R-squared**, and **Nagelkerke R-squared**.

```{r model fit index pseudo R2, echo=TRUE}

library(pscl)
pR2(logistic_model)
```

## Assessing Predictive Ability of Model

In the section above, we examined the fit of the model using our training data. In this section, we test how accurate the model is when predicting the outcome using a new dataset.

For every value, we are predicting a probability of the person leaving (log(odds) = weighted predictors + constant). The log() part of the equation is converting the odds back to a probability, meaning that the output of the model is a guess of the probability that the person will leave.

To assess the predictive ability of our model, we use our model to predict the probability that each person in the testing dataset will leave. We then convert these probabilities to binary outcomes (1 = they left, if the probability is greater than 0.5; 0 = they did not leave, if the probability is less than 0.5).

Then, we evaluate the accuracy by seeing how many outcomes the model got right (i.e., comparing predicted outcomes to actual outcomes). If predicted classes is the same as the actual outcome, it gets counted as 1; else as 0. We calculate the number of outcomes the model got right over the total number of outcomes. This gives us the accuracy of the model on the test set.

```{r assessing model predictive ability, echo=TRUE}

#-----------------------------------------------------------------------------
# Convert Continuous Predictors to Factor Variables In Testing Dataset -------
#-----------------------------------------------------------------------------

# Note that we have to apply the same quuantiles that we calculated from the training dataset to the testing dataset. 

# Calculate quartiles from training data
satisfaction_level_quantiles <- quantile(train_data$satisfaction_level, probs = c(0.25, 0.50, 0.75))
last_evaluation_quantiles <- quantile(train_data$last_evaluation, probs = c(0.25, 0.50, 0.75))
average_monthly_hours_quantiles <- quantile(train_data$average_monthly_hours, probs = c(0.25, 0.50, 0.75))
number_project_quantiles <- quantile(train_data$number_project, probs = c(0.25, 0.50, 0.75))
time_spend_company_quantiles <- quantile(train_data$time_spend_company, probs = c(0.25, 0.50, 0.75))

# Now we use these to categorize the predictors in the testing dataset.

test_data <- test_data %>%
  mutate(
    satisfaction_level_cat = case_when(
      satisfaction_level <= satisfaction_level_quantiles[1] ~ "Low",
      satisfaction_level > satisfaction_level_quantiles[1] & satisfaction_level <= satisfaction_level_quantiles[2] ~ "Lower-Mid",
      satisfaction_level > satisfaction_level_quantiles[2] & satisfaction_level <= satisfaction_level_quantiles[3] ~ "Upper-Mid",
      satisfaction_level > satisfaction_level_quantiles[3] ~ "High"
    ),
    last_evaluation_cat = case_when(
      last_evaluation <= last_evaluation_quantiles[1] ~ "Low",
      last_evaluation > last_evaluation_quantiles[1] & last_evaluation <= last_evaluation_quantiles[2] ~ "Lower-Mid",
      last_evaluation > last_evaluation_quantiles[2] & last_evaluation <= last_evaluation_quantiles[3] ~ "Upper-Mid",
      last_evaluation > last_evaluation_quantiles[3] ~ "High"
    ),
    average_monthly_hours_cat = case_when(
      average_monthly_hours <= average_monthly_hours_quantiles[1] ~ "Low",
      average_monthly_hours > average_monthly_hours_quantiles[1] & average_monthly_hours <= average_monthly_hours_quantiles[2] ~ "Lower-Mid",
      average_monthly_hours > average_monthly_hours_quantiles[2] & average_monthly_hours <= average_monthly_hours_quantiles[3] ~ "Upper-Mid",
      average_monthly_hours > average_monthly_hours_quantiles[3] ~ "High"
    ),
    number_project_cat = case_when(
      number_project <= number_project_quantiles[1] ~ "Low",
      number_project > number_project_quantiles[1] & number_project <= number_project_quantiles[2] ~ "Lower-Mid",
      number_project > number_project_quantiles[2] & number_project <= number_project_quantiles[3] ~ "Upper-Mid",
      number_project > number_project_quantiles[3] ~ "High"
    ),
    time_spend_company_cat = case_when(
      time_spend_company <= time_spend_company_quantiles[1] ~ "Low",
      time_spend_company > time_spend_company_quantiles[1] & time_spend_company <= time_spend_company_quantiles[2] ~ "Lower-Mid",
      time_spend_company > time_spend_company_quantiles[2] & time_spend_company <= time_spend_company_quantiles[3] ~ "Upper-Mid",
      time_spend_company > time_spend_company_quantiles[3] ~ "High"
    )
  )
# Convert to factors
test_data <- test_data %>%
  mutate(across(ends_with("_cat"), as.factor))

#-----------------------------------------------------------------------------
# Test Predictions on Testing Dataset ----------------------------------------
#-----------------------------------------------------------------------------

# Predict on test data
predictions <- predict(logistic_model, newdata = test_data, type = "response")

# Convert probabilities to binary predictions (e.g., 0.5 threshold)
predicted_outcome <- ifelse(predictions > 0.5, 1, 0)

# Evaluate accuracy
accuracy <- mean(predicted_outcome == test_data$left_numeric)
print(paste("Test Set Accuracy:", accuracy))
```

-   **84.17% Accuracy**: This indicates that 84.17% of the predictions made by our model on the test data are correct.

Let's also test the accuracy on the training dataset. We do this to obtain a baseline for comparing the accuracy of the testing dataset - if we get high accuracy on the training set and lower on the test set, it suggests the model does not generalize well to new data. If we have low accuracy on both, it suggests that the model generalizes well but may not be a good model.

```{r accuracy training set, echo=FALSE}

#-----------------------------------------------------------------------------
# Test Predictions on Training Dataset ----------------------------------------
#-----------------------------------------------------------------------------

# Predict on training data
predictions <- predict(logistic_model, newdata = train_data, type = "response")

# Convert probabilities to binary predictions (e.g., 0.5 threshold)
predicted_outcome <- ifelse(predictions > 0.5, 1, 0)

# Evaluate accuracy
accuracy <- mean(predicted_outcome == train_data$left_numeric)
print(paste("Test Set Accuracy:", accuracy))
```

-   **85.04% Accuracy**: This indicates that 85.04% of the predictions made by your model on the test data are correct.

# Summary of Findings from Logistic Regression

### Summary of Key Findings and Recommendations

1.  **Satisfaction Level**: Employees with lower satisfaction levels show significantly higher odds of leaving, particularly those in the "Low" satisfaction category, who have approximately 7.5 times the likelihood of turnover compared to highly satisfied employees. This finding suggests that low satisfaction could be a key risk factor in employee turnover. Addressing this might involve implementing regular feedback mechanisms, such as engagement surveys, to identify sources of dissatisfaction. Targeted initiatives, including flexible work arrangements, clear career growth opportunities, and recognition programs, may help alleviate dissatisfaction, potentially reducing turnover among at-risk employees.

2.  **Performance on Last Evaluation**: Employees with high performance evaluations show a somewhat elevated likelihood of leaving compared to those with moderate performance. This trend could indicate unmet expectation, increased job offers from other companies, or a lack of recognition for high achievers. All in all, it warrants further investigation. To support high performers, organizations might consider offering tailored recognition, mentorship, and career development paths that address their ambitions. These strategies could help reinforce high performers' commitment, reducing the risk of losing valuable talent by helping to ensure they feel appreciated and engaged in their roles.

3.  **Workload (Number of Projects, Average Monthly Hours):** The analysis suggests that employees with a moderate workload (i.e., either a moderate number of projects or a moderate average monthly hours) tend to have lower turnover odds than those handling a high workload. This trend could imply that an excessive workload might contribute to burnout, making project allocation an important consideration for retention efforts. At the same time, those with a low workload (i.e., those assigned a low number of projects or low average monthly hours) are only somewhat less likely to leave compared to those assigned a high number of projects. This could suggest that under-utilization could be a risk factor for disengagement. Aligning project assignments with employees' strengths and capacities and ensuring that workloads are balanced could be a good starting point to helping reduce this risk factor.

4.  **Time Spent at Company (Tenure):** Employees with shorter tenure exhibit lower turnover odds compared to those with a longer tenure, which may indicate that newer employees feel more engaged initially. However, as tenure increases, employees might experience a dip in engagement or growth opportunities. This may reflect a "honeymoon-hangover" effect (Boswell et al., 2005). To counteract this, organizations could implement strategies to sustain long-term engagement, such as offering role changes, professional development, and growth opportunities.

### Next Steps

-   **Follow Up Mediation Study:** To gain deeper insights into the drivers of turnover, we recommend a follow-up study using mediation analysis to examine whether factors like workload and time spent at the company influence turnover through their effects on job satisfaction. Specifically, we propose a study design with three separate data collection points to reduce common method variance and strengthen causal interpretation. At **Time 1**, predictors such as workload and tenure would be measured; at **Time 2**, several months later, job satisfaction would be assessed; and at **Time 3**, turnover intentions or actual turnover data would be collected. This temporally separated design would clarify the extent to which job satisfaction mediates the relationship between workload, tenure, and turnover, providing actionable insights on whether improving satisfaction could mitigate turnover risks. These findings would inform targeted retention strategies, such as adjusting workloads or enhancing satisfaction-focused initiatives.

-   **Investigate Alternative Modeling Approaches:** n addition to exploring mediation through a temporally separated study, another valuable next step would be to investigate alternative modeling approaches, such as neural networks (e.g., multi-layer perceptrons), to predict turnover. These models could capture more complex, non-linear relationships between predictors like workload, tenure, and satisfaction that traditional logistic regression might miss. Implementing neural networks alongside traditional models could enhance predictive accuracy and help uncover deeper patterns in the data, offering a more robust foundation for targeted retention strategies.

# Neural Network: Multi-Layer Perceptron

::: {.callout-note collapse="true" icon="false"}
## Background Explanation

A neural network is a model inspired by how the human brain works. It consists of "neurons" (also called nodes) connected in layers. In a **Multi-Layer Perceptron (MLP)**, the network is organized into different types of "layers":

-   **Input layer:** Receives the input data (e.g., the predictors)

-   **Hidden layer(s):** Layers between the input and output layers, where the network learns to detect patterns in the data.

-   **Output layer:** Produces the final prediction. For classification problems, the output layer typically contains neurons representing each class (e.g., "stay" or "leave"), and the network outputs a probability for each class. For regression problems, there’s often a single neuron in the output layer, providing a continuous value.

**How Neurons Process Inputs**

In a neural network, each "neuron" (or node) in the network receives some inputs, performs calculations, and then decides (1) if the neuron will "fire" (i.e., whether or not to pass its output on to the next layer, and (2) how much of the signal to send.

Each neuron receives inputs from the previous layer (or, in the case of the input layer, the original input data), multiplies each input by a weight that represents its importance, and adds a bias term. The neuron then sums these weighted inputs, and the result is passed through an **activation function** to produce the neuron’s output, which will be sent to the neurons in the next layer.

**Hidden Layers**

The hidden layers are where the network learns and detects patterns. These layers are often referred to as the "black box" of the network because we can’t directly observe the patterns they’re learning.

In research terms, the hidden layers essentially act as **"mediators"**; they take in the inputs and don't just pass the information to the output, but instead "mediate" by processing, transforming, and combining the inputs to build complex representations that can help make the final prediction.

Just as mediators in research help us understand the process behind a relationship, hidden layers help the MLP capture and represent complex underlying processes in the data. They can be thought of as extracting intermediate representations that explain part of the relationship between input data and output prediction. Each hidden layer builds on the patterns detected by the previous layer, making it possible to capture more complex and non-linear relationships.

You can have more than one hidden layer, effectively creating multiple "mediator steps" in the relationship. Each additional layer allows the network to learn and represent more abstract patterns, enabling it to capture even more complex relationships in the data.

**Non-Linearity and Activation Functions**

An important part of neural networks is that they **don’t assume linearity** in the underlying relationship. By stacking multiple layers and using **non-linear activation functions**, an MLP can capture complex, non-linear relationships in the data, allowing it to model a wider range of patterns.

**Activation function**: An activation function is a mathematical function applied to the output of each neuron (node) in a neural network layer. After a neuron receives inputs, it calculates a weighted sum (adding together the inputs multiplied by their weights and adding a bias). This weighted sum is then passed through an activation function, which determines the neuron’s output.

The purpose of the activation function is to introduce non-linearity into the model. Without it, the entire network would behave like a single linear transformation, regardless of how many layers it has. By using activation functions, we allow the network to learn and represent non-linear relationships, which are common in real-world data.

**Training the Neural Network**

Imagine a neural network is making predictions, like guessing whether an employee will stay or leave based on certain inputs. When the network makes a guess, we can **compare the guess to the actual answer** and see how far off it was. This difference is called the **error**.

The process of training a neural network involves **reducing this error** by adjusting the importance (or "weights") the network assigns to each input feature. This is done through a process called **backpropagation**. The network works backwards through its layers to figure out which weights contributed most to the error and need adjusting. Then it adjusts the weights to reduce the error.

This process of making predictions, checking errors, and adjusting weights is repeated many times across the dataset. Each time, the network improves slightly, learning which inputs are more important for accurate predictions.

Over many rounds, the network gets better at making predictions because it has fine-tuned the weights, allowing it to minimize the error and better understand the relationship between inputs and the desired output.
:::

::: {.callout-tip collapse="true" icon="false"}
## Chef Analogy: Classification

Let's say we have a chef at a restaurant, who is trying to figure out which type of salad they have in front of them based on the ingredients and how they’re arranged, without actually adjusting the salad or iteratively refining it.

**Goal:** The chef’s job is to identify the type of salad (e.g., Caesar, Greek, or Garden) based on fixed ingredients and presentation, not to make or modify it.

**Ingredients as Input Features:** The chef observes a set of ingredients in the salad—perhaps some lettuce, tomatoes, olives, croutons, and feta cheese. These ingredients represent input features.

**Layers as Pattern Recognition Stages:** The chef mentally processes these ingredients through several stages (layers) to make sense of the salad. At each stage, they notice different characteristics:

-   **Layer 1 (Basic Identification):** The chef identifies individual ingredients like lettuce, olives, and croutons.

-   **Layer 2 (Pattern Recognition):** The chef begins to recognize ingredient combinations and patterns, such as the presence of olives and feta, which often appear together in a Greek salad.

-   **Layer 3 (Final Classification):** After processing all the details, the chef identifies which known salad the ingredients match best.

```{=html}
<!-- -->
```
-   **Output as Final Decision:** Based on all observed features and patterns, the chef makes a final decision—perhaps identifying the salad as likely to be "Greek Salad", and not likely to be a Caesar salad or a garden salad.
:::

::: {.callout-tip collapse="true" icon="false"}
## Chef Analogy: Reinforcement Learning

Imagine a neural network as a restaurant kitchen, with many chefs and different stations. The goal is to take a basic set of ingredients and transform them through multiple stations (layers) to create a final dish (for example, a salad) that the customer will find delicious. Each **station (layer)** is responsible for a specific transformation of ingredients, and each **chef (neuron)** within a station has a specific role in that transformation.

**Starting with the Ingredients (Inputs)**:

-   At the start of the process, we have all our **ingredients** — things like lettuce, tomatoes, cucumbers, croutons, olive oil, honey, salt, etc. These ingredients represent the **input features** of the neural network.

**Chefs as Neurons and Stations as Layers**

-   **Each chef (neuron)** is stationed at a specific part of the process **(layer).**

-   Each station (layer) has an overall job to do in creating the salad, but each chef within that station also has a specific job to do.

    -   For example, station 1 might be all about prepping the ingredients into salad components:

        -   Chef 1 in Layer 1 might prep the salad ingredients

        -   Chef 2in Layer 1 prepares the dressing

        -   Chef 3 in Layer 1 prepares the add-ons (e.g., cheese, croutons, etc.)

    -   Station 2 might then be about combining the components together from station 1 to form a specific salad.

**Layer 1: Prepping the Salad Components**

-   Each chef in layer 1 has access to all the ingredients - but depending on their task (e.g., preparing the dressing, preparing the vegetables), they may use none, a little, or a lot of each ingredient. For example, the chef making the dressing may use a lot of olive oil and vinegar (high weights) and little or no lettuce (low or zero weights). The **amount of each ingredient** that each chef uses represents the **weight** of each input in a neuron’s regression equation.

-   By using different amounts of each ingredient (weights), each chef creates a **unique version of their output**. The **output of each neuron** is like the result of a weighted regression equation, where the ingredients (inputs) are combined according to the weights each chef gives to each ingredient.

**Activation Function: Taste Testing**

After each chef in Layer 1 has prepared their specific component (dressing, vegetables, or add-ons) using different amounts of each ingredient (weights), they perform a **taste test**. This taste test represents the **activation function** in a neural network.

-   The taste test allows each chef to decide if their component is balanced and flavorful enough to move onto the next stage.

-   If the flavors in a chef’s component are too overpowering or don’t fit well, the chef might adjust or hold back certain ingredients — similar to how the activation function might set certain values to zero or scale them differently.

-   Only the components that pass the taste test (activation function) are passed on to the next layer for further combination.

**Layer 2: Combining the Components into Different Salads**

In Layer 2, the goal is to **combine the prepared components** from Layer 1 to start forming the overall salad. Each chef in this layer receives the outputs from all chefs in Layer 1 (the prepared components: greens, dressing, add-ons) and combines them, but each chef in Layer 2 has a unique approach to this combination.

-   **Chef 1 in Layer 2** might focus on creating garden salads.

-   Chef 2 in Layer 2 might focus on creating Ceasar salads.

-   Chef 3 in Layer 2 might focus on creating Greek salads.

Each chef in this layer is **combining the outputs from Layer 1 differently**, using different proportions or adding different elements to create unique versions of the salad. These variations in **proportions and emphasis** reflect how **neurons in a hidden layer combine weighted inputs from the previous layer** to create different outputs based on their specific task.

**Activation Function: Taste-Testing in Layer 2**

After combining the components, each chef **taste-tests** their version of the salad to make sure the flavors are balanced.

-   If any of the salads have **too much of one flavor**, they adjust by adding or removing ingredients (e.g., more croutons, less dressing). This adjustment represents the **activation function** again, helping refine the output based on the chef’s specific recipe./

-   Only the versions of the salad that pass the taste test are **passed on to the next layer**.

**Layer 3: Decision by Head Chefs**

In **Layer 3**, the final task is to **evaluate and finalize the salad**. The **head chef** takes all the versions from Layer 2 and decides which one to serve, or perhaps combines aspects of the different salads into one perfect version.

-   **Chef 1 in Layer 3** might taste each salad and decide which one is most balanced, choosing the best version to serve.

-   **Chef 2 in Layer 3** might add any final tweaks, such as a dash of extra seasoning or a fresh herb garnish, to make the salad complete.

-   **Chef 3 in Layer 3** could combine elements from multiple versions — like mixing the crispness of the garden salad with the creaminess of the Caesar salad — to create a final version.

This **final decision** reflects the **output layer** of the neural network, where the model makes its **final prediction** or decision based on the outputs from the previous layers.

**Final Output: The Completed Salad**

Once all layers have processed their inputs and made adjustments, the final version of the salad is ready to be served to the customer. This **final salad** represents the **neural network’s prediction**, a well-balanced result created from the outputs of each layer (station).

-   If the head chef (output layer) chooses the perfect version of the salad, it represents a successful prediction.

**Learning**

-   At the end of the night, the chefs look at the feedback collected from customers about the salad. If the salad needs adjustments, the chefs will refine their approach based on feedback, just as a neural network refines its weights during training.

-   If the overall feedback was that the salad was too rich, the individuals chefs will consider how to improve on that (e.g., the chef in charge of preparing dressing might use less oil; the chef in charge of cutting vegetables might use more cucumbers and less avocado, etc.).
:::

```{r multi layer perceptron neural network, echo=TRUE}

pacman::p_load(nnet, NeuralNetTools)

# Fitting the MLP model - by default, nnet() uses the sigmoid() function for the activation function for the hidden layers - this means that each hidden neuron's output will be passed through a sigmoid function, which introduces non-linearity to the model.

mlp_model <- nnet(left ~ satisfaction_level + last_evaluation + 
              average_monthly_hours + number_project + 
              time_spend_company, 
              data = train_data, 
              size = 5, # specifies the number of neurons in the hidden layer
              linout = FALSE, # this specifies classification not regression
              maxit=5000,
              trace=TRUE)

# Predict on test data
predictions <- predict(mlp_model, test_data, type = "raw")
predicted_outcomes <- ifelse(predictions > 0.5, 1, 0)

# Evaluate accuracy
accuracy <- mean(predicted_outcomes == test_data$left)
print(paste("Test Set Accuracy:", accuracy))

plotnet(mlp_model)  # Visualizes the structure of the neural network


```
